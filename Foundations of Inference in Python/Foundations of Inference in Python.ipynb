{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a603519-0bf1-44fc-b604-9251dc69e170",
   "metadata": {},
   "source": [
    "## Foundations of Inference in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a32ec-2923-4fa3-bd99-b44f9b209721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random starting row number, not including the last 90 rows\n",
    "initial_row_number = np.random.choice(range(len(btc_sp_df) - 90))\n",
    "\n",
    "# Use initial_row_number to select the next 90 rows from that row number\n",
    "sample_df = btc_sp_df.iloc[initial_row_number:(initial_row_number + 90)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3b658-842c-4f99-8244-5a4c7a6ad65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random starting row number, not including the last 90 rows\n",
    "initial_row_number = np.random.choice(range(btc_sp_df.shape[0] - 90))\n",
    "\n",
    "# Use initial_row_number to select the next 90 rows from that row number\n",
    "sample_df = btc_sp_df.iloc[initial_row_number:(initial_row_number + 90)]\n",
    "\n",
    "# Use sample_df to compute the percent increase in Close_SP500\n",
    "sp500_pct_change = ((sample_df['Close_SP500'].iloc[-1] - sample_df['Close_SP500'].iloc[0]) / sample_df['Close_SP500'].iloc[0]) * 100\n",
    "\n",
    "# Use sample_df to compute the percent increase in Close_BTC\n",
    "btc_pct_change = ((sample_df['Close_BTC'].iloc[-1] - sample_df['Close_BTC'].iloc[0]) / sample_df['Close_BTC'].iloc[0]) * 100\n",
    "\n",
    "print('SP500: ', sp500_pct_change, '\\n', 'BTC: ', btc_pct_change)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6caf66e-21d3-4b0d-a087-da5c371f224d",
   "metadata": {},
   "source": [
    "### Sampling and point estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dedc831-055f-42de-bab7-69400b70846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random starting row number, not including the last 90 rows\n",
    "initial_row_number = np.random.choice(range(btc_sp_df.shape[0] - 90))\n",
    "\n",
    "# Use initial_row_number to select the next 90 rows from that row number\n",
    "sample_df = btc_sp_df.iloc[initial_row_number:(initial_row_number + 90)]\n",
    "\n",
    "# Use sample_df to compute the percent increase in Close_SP500\n",
    "sp500_pct_change = ((sample_df['Close_SP500'].iloc[-1] - sample_df['Close_SP500'].iloc[0]) / sample_df['Close_SP500'].iloc[0]) * 100\n",
    "\n",
    "# Use sample_df to compute the percent increase in Close_BTC\n",
    "btc_pct_change = ((sample_df['Close_BTC'].iloc[-1] - sample_df['Close_BTC'].iloc[0]) / sample_df['Close_BTC'].iloc[0]) * 100\n",
    "\n",
    "print('SP500: ', sp500_pct_change, '\\n', 'BTC: ', btc_pct_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad42d3-c2cb-4a2c-9b5c-7817b1531199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9306211-47ce-42b7-97c9-1d361ac25060",
   "metadata": {},
   "source": [
    "### Repeated sampling, point estimates and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb496d5-f37a-476f-8693-02f33482d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a for loop which repeats the sampling ten times\n",
    "for i in range(10):\n",
    "    # Select a random starting row number\n",
    "    initial_row_number = np.random.choice(range(btc_sp_df.shape[0] - 90))\n",
    "    # Select the next 90 rows after the starting row\n",
    "    sample_df = btc_sp_df.iloc[initial_row_number:initial_row_number + 90]\n",
    "    # Compute the percent change in closing price of BTC and print it\n",
    "    btc_pct_change = (sample_df.iloc[0]['Close_BTC'] - sample_df.iloc[-1]['Close_BTC']) / sample_df.iloc[0]['Close_BTC']\n",
    "    print(btc_pct_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215dec7d-8129-4c71-94e3-2e183cef618f",
   "metadata": {},
   "source": [
    "### Visualizing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f972283-0333-42c7-9429-2d1234449c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of percent changes\n",
    "plt.hist(btc_pct_change_list, bins=15, density=True)\n",
    "# Set the x-axis label\n",
    "plt.xlabel('BTC 90-day percent change')\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Percent of samples')\n",
    "# Set the title\n",
    "plt.title('Sampling distribution of BTC 90-day change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056a4cf-36e2-4d99-a149-e85f78795b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples to take and store the sample means\n",
    "num_samples = 200\n",
    "sample_means = []\n",
    "\n",
    "# Write a for loop which repeats the sampling num_samples times\n",
    "for i in range(num_samples):\n",
    "  # Select 500 random Close_SP500 prices \n",
    "  sp500_sample = np.random.choice(btc_sp_df['Close_SP500'], size=500)\n",
    "  # Compute mean closing price and save it to sample_means\n",
    "  sample_means.append(sp500_sample.mean())\n",
    "    \n",
    "plt.hist(sample_means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f0204-64dc-4a26-8b8c-0cc18e484cf1",
   "metadata": {},
   "source": [
    "### Calculate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c34398-25e5-46ae-be2e-e9b2adfedeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of 500 random days\n",
    "sample_closing = np.random.choice(btc_sp_df['Close_SP500'], size=500)\n",
    "\n",
    "# Calculate the mean of the sample\n",
    "sample_mean = sample_closing.mean()\n",
    "\n",
    "# Calculate the standard error of the sample\n",
    "sample_se = sample_closing.std() / np.sqrt(sample_closing.shape[0])\n",
    "\n",
    "# Calculate a 95% confidence interval using this data\n",
    "stats.norm.interval(alpha=0.95,\n",
    "                   loc=sample_mean,\n",
    "                   scale=sample_se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc645440-f370-46c5-90bf-8eddd660be64",
   "metadata": {},
   "source": [
    "### Drawing conclusions from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58815ecf-836a-41bc-94f9-ff78b8c519d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1: Select a random sample of 500 rows\n",
    "sample_df = btc_sp_df.sample(n=500)\n",
    "\n",
    "# Compute a 95% confidence interval for the closing price of SP500\n",
    "sample_ci = stats.norm.interval(alpha=0.95, \n",
    "                            loc=sample_df['Close_SP500'].mean(), \n",
    "                            scale=sample_df['Close_SP500'].std()/np.sqrt(len(sample_df)))\n",
    "\n",
    "print(sample_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bf5de-1b9d-49c1-a35c-d8bb7ff51ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1: Select a random sample of 500 rows\n",
    "sample_df = btc_sp_df.sample(n=500)\n",
    "\n",
    "sample_ci = stats.norm.interval(alpha=0.95, \n",
    "\tloc=sample_df['Close_SP500'].mean(), \n",
    "\tscale=sample_df['Close_SP500'].std()/np.sqrt(500))\n",
    "\n",
    "# Sample 2: Select the first 500 rows\n",
    "first_500_df = btc_sp_df.iloc[:500]\n",
    "\n",
    "# Compute a 95% confidence interval for the closing price of SP500\n",
    "first_500_ci = stats.norm.interval(alpha=0.95,loc=first_500_df['Close_SP500'].mean(),scale=first_500_df['Close_SP500'].std()/np.sqrt(500))\n",
    "\n",
    "print(first_500_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b35e5-68a8-4f5e-837a-6c63d7c977b1",
   "metadata": {},
   "source": [
    "### Testing for normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990c3fe-dde8-43e0-a4bc-bf60429e91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the employees' \"Years of Employment\"\n",
    "salary_df['Years of Employment'].plot(kind=\"hist\")\n",
    "plt.show()\n",
    "\n",
    "# Conduct an Anderson-Darling test using the years of employment from salary_df\n",
    "result = stats.anderson(salary_df['Years of Employment'])\n",
    "\n",
    "# Print which critical values the test statistic is greater than the critical values\n",
    "print(result.statistic > result.critical_values)\n",
    "\n",
    "# Print the significance levels at which the null hypothesis is rejected\n",
    "print(result.significance_level[result.statistic > result.critical_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a049f7-2d52-4011-9c29-8d55460572af",
   "metadata": {},
   "source": [
    "### Distribution of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2503dec-f282-4b71-a3a4-acc9bfc45759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error as actual minus predicted salary\n",
    "error = salaries-preds\n",
    "\n",
    "# Plot the errors as a histogram\n",
    "plt.hist(error)\n",
    "plt.show()\n",
    "\n",
    "# Conduct an Anderson-Darling test using the years of experience\n",
    "result = stats.anderson(error)\n",
    "\n",
    "# Find where the result is significant\n",
    "print(result.significance_level[result.statistic > result.critical_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c2361-ceb6-4b63-9abd-717076149527",
   "metadata": {},
   "source": [
    "### Fitting a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21febc3-cebf-472f-86c3-1215b7ce6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a normal distribution to the data\n",
    "mu, std = stats.norm.fit(salary_df['Years of Employment'])\n",
    "\n",
    "# Compute the percentage of employees with less than 10 years experience\n",
    "percent = stats.norm.cdf(10, loc=mu, scale=std)\n",
    "\n",
    "# Print out this percentage\n",
    "print(percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aebeae-f016-4ca3-a6ca-1c0c24a3f950",
   "metadata": {},
   "source": [
    "### Testing for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7efdf-5d62-4e43-b914-62c29bce71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line graph showing the rents for both San Francisco and Las Vegas\n",
    "plt.plot(dates, houston_rents, label='Houston')\n",
    "plt.plot(dates, lasvegas_rents, label='Las Vegas')\n",
    "plt.show()\n",
    "\n",
    "# Compute the Pearson correlation coefficient R, as well as the p-value\n",
    "r, p_value = stats.pearsonr(houston_rents, lasvegas_rents)\n",
    "\n",
    "# Print if the p-value is less than alpha = 5%\n",
    "print(p_value<0.05)\n",
    "\n",
    "# Print out R-squared\n",
    "print(r**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94ca9c-aac2-4484-94f9-96cf5f10d56d",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b30bc-b7d8-4338-b804-92638b21c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all but the first twelve rents\n",
    "la_rents_initial = la_rents[12:]\n",
    "\n",
    "# Select all but the last twelve rents (12 month lag)\n",
    "la_rents_lag = la_rents[:-12]\n",
    "\n",
    "# Compute the correlation between the initial values and the lagged values\n",
    "r, p_value = stats.pearsonr(la_rents_initial, la_rents_lag)\n",
    "\n",
    "# Check if the p-value is significant at the 5% level\n",
    "print(p_value<0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f167f5-4f08-4a0f-960a-b0f0b8e9198c",
   "metadata": {},
   "source": [
    "### Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83804ca0-3ef9-49f9-9479-82a2a40dfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation between Houston and Las Vegas\n",
    "r, p_value = stats.pearsonr(houston_rents,lasvegas_rents)\n",
    "\n",
    "# Print R-squared\n",
    "print(r**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43bca3-bf0a-4a97-8ee5-f131f45cbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select each industry separately\n",
    "biotech_df = investments_df[investments_df['market']=='Biotechnology']['funding_total_usd']\n",
    "enterprise_df = investments_df[investments_df['market']=='Enterprise Software']['funding_total_usd']\n",
    "health_df = investments_df[investments_df['market']=='Health Care']['funding_total_usd']\n",
    "\n",
    "# Conduct Levene tests for equal variance between funding_total_usd for all pairs of industries\n",
    "statistic_1, p_value_1 = stats.levene(biotech_df, enterprise_df)\n",
    "statistic_2, p_value_2 = stats.levene(biotech_df, health_df)\n",
    "statistic_3, p_value_3 = stats.levene(enterprise_df, health_df)\n",
    "\n",
    "# Print if the p-value is significant at the 5% level\n",
    "print(p_value_1 < 0.05)\n",
    "print(p_value_2 < 0.05)\n",
    "print(p_value_3 < 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d00c56f-f03d-49a1-9935-7a12fb1e3799",
   "metadata": {},
   "source": [
    "### Normality of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fee619-cd29-4e22-b2cb-7ba8db3043b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the funding for each industry\n",
    "biotech_df.plot(kind='hist', alpha=0.33)\n",
    "enterprise_df.plot(kind='hist', alpha=0.33)\n",
    "ecommerce_df.plot(kind='hist', alpha=0.33)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631bc76-bee7-42d4-9dc0-72686c481db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the log funding for each industry\n",
    "np.log(biotech_df['funding_total_usd']).plot(kind='hist', color='blue', alpha=0.33)\n",
    "np.log(enterprise_df['funding_total_usd']).plot(kind='hist', color='green', alpha=0.33)\n",
    "np.log(ecommerce_df['funding_total_usd']).plot(kind='hist', color='red', alpha=0.33)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfcc85-0408-44c9-8644-51f1dc4b720e",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251defd9-b531-4c84-be1b-dbfffdf79e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "biotech_log_funding = np.log(biotech_df['funding_total_usd'])\n",
    "enterprise_log_funding = np.log(enterprise_df['funding_total_usd'])\n",
    "health_log_funding = np.log(health_df['funding_total_usd'])\n",
    "\n",
    "# Conduct a one-way ANOVA test to compare the log-funding\n",
    "s, p_value = stats.f_oneway(biotech_log_funding, enterprise_log_funding, health_log_funding)\n",
    "\n",
    "# Print if the p-value is significant at 5%\n",
    "print(p_value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccdb9c-5e74-4aea-bc7b-db49410df01e",
   "metadata": {},
   "source": [
    "### Comparing rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c848b4-c299-4d3e-9d0c-03f915b6e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Kendall's tau between the THEW and ARW rankings\n",
    "tau_thew_arw, p_value_thew_arw = stats.kendalltau(rankings_df['thew_rank'], rankings_df['arw_rank'])\n",
    "\n",
    "# Compute Kendall's tau between the THEW and CW rankings\n",
    "tau_thew_cw, p_value_thew_cw = stats.kendalltau(rankings_df['thew_rank'], rankings_df['cw_rank'])\n",
    "\n",
    "# Compute Kendall's tau between the ARW and CW rankings\n",
    "tau_arw_cw, p_value_arw_cw = stats.kendalltau(rankings_df['arw_rank'], rankings_df['cw_rank'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b78a5-a84e-4717-bf94-75267b44e66c",
   "metadata": {},
   "source": [
    "### Comparing medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcc637-756a-4179-92d9-19b3b850d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the CW total score\n",
    "rankings_df['cw_score'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aaf693-b660-41ed-89fc-cafe1d9364db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the CW total score\n",
    "rankings_df['cw_score'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of the ARW total score\n",
    "rankings_df['arw_score'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a62b6d-1289-4dbd-9aa5-dc2cbb9ec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the CW total score\n",
    "rankings_df['cw_score'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of the ARW total score\n",
    "rankings_df['arw_score'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Conduct a Mood's median test comparing cw_score and arw_score\n",
    "s, p_value, med, table = stats.median_test(rankings_df['cw_score'], rankings_df['arw_score'])\n",
    "\n",
    "# Check if the p-value is significant at 5%\n",
    "print(p_value < 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbef19-549c-48d3-b209-344c87208f1e",
   "metadata": {},
   "source": [
    "### Effect size for means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121620d-e0c7-476f-a9fc-3ac7cdc0832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all investments from rounds 1 and 2 separately\n",
    "round1_df = investments_df[investments_df['funding_rounds'] == 1]\n",
    "round2_df = investments_df[investments_df['funding_rounds'] == 2]\n",
    "\n",
    "# Calculate the standard deviation of each round and the number of companies in each round\n",
    "round1_sd = round1_df['funding_total_usd'].std()\n",
    "round2_sd = round2_df['funding_total_usd'].std()\n",
    "round1_n = round1_df.shape[0]\n",
    "round2_n = round2_df.shape[0]\n",
    "\n",
    "# Calculate the pooled standard deviation between the two rounds\n",
    "pooled_sd = np.sqrt(((round1_n - 1) * round1_sd ** 2 + (round2_n - 1) * round2_sd ** 2) / (round1_n + round2_n - 2))\n",
    "\n",
    "# Calculate Cohen's d\n",
    "d = (round1_df['funding_total_usd'].mean() - round2_df['funding_total_usd'].mean()) / pooled_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e1b20-6728-4ece-a1a4-abc855169329",
   "metadata": {},
   "source": [
    "### Effect size for correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb2ff7-53df-4aaa-988c-100d13ee045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the volatility of Bitcoin\n",
    "btc_sp_df['Volatility_BTC'] = (btc_sp_df['High_BTC'] - btc_sp_df['Low_BTC']) / btc_sp_df['Close_BTC']\n",
    "\n",
    "# Compute the volatility of the S&P500\n",
    "btc_sp_df['Volatility_SP500'] = (btc_sp_df['High_SP500'] - btc_sp_df['Low_SP500']) / btc_sp_df['Close_SP500']\n",
    "\n",
    "# Compute and print R^2 between the volatility of BTC and SP500\n",
    "r_volatility, p_value_volatility = stats.pearsonr(btc_sp_df['Volatility_BTC'], btc_sp_df['Volatility_SP500'])\n",
    "print('R^2 between volatility of the assets:', r_volatility ** 2)\n",
    "\n",
    "# Compute and print R^2 between the volatility of BTC and the closing price of BTC\n",
    "r_closing, p_value_closing = stats.pearsonr(btc_sp_df['Volatility_BTC'], btc_sp_df['Close_BTC'])\n",
    "print('R^2 between closing price and volatility of BTC:', r_closing ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372c6fa-013a-4bfe-a2e8-59e229c5769a",
   "metadata": {},
   "source": [
    "### Effect size for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abc4ce-23e5-4b25-8799-9de41d1a59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the chi-squared statistic\n",
    "chi2, p, d, expected = stats.chi2_contingency(employees_df)\n",
    "\n",
    "# Compute the DOF using the number of rows and columns\n",
    "dof = min(employees_df.shape[0] - 1, employees_df.shape[1] - 1)\n",
    "\n",
    "# Compute the total number of people\n",
    "n = np.sum(employees_df.values)\n",
    "\n",
    "# Compute Cramer's V\n",
    "v = np.sqrt((chi2 / n) / dof)\n",
    "\n",
    "print(\"Cramer's V:\", v, \"\\nDegrees of freedom:\", dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035d57c-2dc4-42ee-88f8-edea86a48015",
   "metadata": {},
   "source": [
    "### Multiple comparisons problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc412133-ba91-48fc-9561-6d396a0cd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of rows and initialize n_significant\n",
    "n_rows = police_salaries_df.shape[0]\n",
    "n_significant = 0\n",
    "\n",
    "# For loop which generates n_rows random numbers 1000 times\n",
    "for i in range(1000):\n",
    "  random_nums = np.random.uniform(size=n_rows)\n",
    "  # Compute correlation between random_nums and police salaries\n",
    "  r, p_value = stats.pearsonr(police_salaries_df['Annual Salary'], random_nums)\n",
    "  # If the p-value is significant at 5%, increment n_significant\n",
    "  if p_value < 0.05:\n",
    "    n_significant += 1\n",
    "    \n",
    "print(n_significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891782f6-0e94-4dc0-b51d-f496119da212",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bonferonni-Holm correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a3d4e-f779-46e9-81d8-3f65d65e2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Bonferroni-corrected alpha\n",
    "p_values=1000\n",
    "bonf_alpha = 0.05 / int(p_values)\n",
    "\n",
    "# Check how many p-values were significant at this level\n",
    "significant_p_values = np.sum(p_values < bonf_alpha)\n",
    "\n",
    "print(f\"Bonferroni-corrected alpha: {bonf_alpha}\")\n",
    "print(f\"Number of significant p-values: {significant_p_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ec113-dbbf-4260-9b8e-850be4c7ac44",
   "metadata": {},
   "source": [
    "### Computing power and sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15c328-d73e-4579-aec7-f89d75d21577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ratio of games to advertising companies\n",
    "games_ads_ratio = len(games_acquired_avg_funding) / len(ads_acquired_avg_funding)\n",
    "\n",
    "# Compute the power of the test\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "power = TTestIndPower().power(effect_size=ads_games_cohensd, \n",
    "                              nobs1=len(ads_acquired_avg_funding),\n",
    "                              alpha=0.05,\n",
    "                              ratio=games_ads_ratio)\n",
    "\n",
    "print(\"Power of the test:\", power)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538b364-6b9c-4efc-a75d-275d6ee02872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ratio of games to advertising companies\n",
    "games_ads_ratio = games_n / ads_n\n",
    "\n",
    "TTestIndPower().power(effect_size=ads_games_cohensd, \n",
    "                      nobs1=ads_n,\n",
    "                      alpha=0.05,\n",
    "                      ratio=games_ads_ratio)\n",
    "\n",
    "# Solve for the sample size needed to achieve a power of 80%\n",
    "nobs1 = TTestIndPower().solve_power(effect_size=ads_games_cohensd, \n",
    "                                    nobs1=None,\n",
    "                                    alpha=0.05,\n",
    "                                    power=0.8)\n",
    "\n",
    "# Print the number of participants needed in one group\n",
    "print(nobs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd3fe1-cd57-4d95-986b-b705eab97257",
   "metadata": {},
   "source": [
    "### Bootstrap confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dae4fb-e1b4-4059-8980-f14cced1110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the daily percent change of each asset\n",
    "btc_sp_df['Pct_Daily_Change_BTC'] = (btc_sp_df['Open_BTC'] - btc_sp_df['Close_BTC']) / btc_sp_df['Open_BTC']\n",
    "btc_sp_df['Pct_Daily_Change_SP500'] = (btc_sp_df['Open_SP500'] - btc_sp_df['Close_SP500']) / btc_sp_df['Open_SP500']\n",
    "\n",
    "# Write a function which returns the correlation coefficient\n",
    "def pearson_r(x, y):\n",
    "    return stats.pearsonr(x, y)[0]\n",
    "\n",
    "# Compute a bootstrap confidence interval\n",
    "ci = stats.bootstrap((btc_sp_df['Pct_Daily_Change_BTC'], btc_sp_df['Pct_Daily_Change_SP500']), \n",
    "                     statistic=pearson_r, \n",
    "                     vectorized=False, paired=True, n_resamples=1000, random_state=1)\n",
    "\n",
    "print(ci.confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab5b06-d73a-48bd-9676-bba801c14129",
   "metadata": {},
   "source": [
    "### Bootstrapping vs. normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd224c-c01a-4140-a26d-2d6a0ee93da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the Analytics companies by looking at the market column\n",
    "analytics_df = investments_df[investments_df['market'] == 'Analytics']\n",
    "\n",
    "# Confidence interval using the stats.norm function\n",
    "norm_ci = stats.norm.interval(alpha=0.95,\n",
    "                              loc=analytics_df['private_equity'].mean(),\n",
    "                              scale=analytics_df['private_equity'].std() / np.sqrt(len(analytics_df)))\n",
    "\n",
    "# Construct a bootstrapped confidence interval\n",
    "bootstrap_ci = stats.bootstrap(data=(analytics_df['private_equity'], ),\n",
    "                               statistic=np.mean)\n",
    "\n",
    "print('Normal CI:', norm_ci)\n",
    "print('Bootstrap CI:', bootstrap_ci.confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d6a77-0f2f-43bf-bf70-013b4e89ab2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fisher's method in SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10f3be-9b6d-4d60-90b1-624fe4be3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the combined p-value and the p-value for this test\n",
    "test_statistic, p_value = stats.combine_pvalues(p_values)\n",
    "\n",
    "# Print out the p-value for the test\n",
    "print('Test p-value = ', p_value)\n",
    "\n",
    "# Print out if the p-value is signifcant at 5%\n",
    "print(p_value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ab1ac-8021-422a-9b85-df05237ecd87",
   "metadata": {},
   "source": [
    "### Permutation tests for correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4051b1-d09c-4a78-8347-ffc9e0568aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which returns the Pearson R value\n",
    "def statistic(x, y):\n",
    "\treturn stats.pearsonr(x, y)[0]\n",
    "\n",
    "# Define the data as the percent daily change from each asset\n",
    "data = (btc_sp_df['Pct_Daily_Change_BTC'], btc_sp_df['Pct_Daily_Change_SP500'])\n",
    "\n",
    "# Compute a permutation test for the percent daily change of each asset\n",
    "res = stats.permutation_test(data, statistic, n_resamples=1000,\n",
    "                             vectorized=False, alternative='greater')\n",
    "\n",
    "# Print if the p-value is significant at 5%\n",
    "print(res.pvalue < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a40e8-c55e-425d-80f8-516e9afc312b",
   "metadata": {},
   "source": [
    "### Analyzing skewed data with a permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6fb299-7566-4477-abb6-783c268cd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a \"statistic\" function which calculates the difference in means\n",
    "def statistic(funding_group_1, funding_group_2):\n",
    "    return np.mean(funding_group_1) - np.mean(funding_group_2)\n",
    "\n",
    "# Conduct a permutation test using 100 resamples\n",
    "perm_result = stats.permutation_test((analytics_df['funding_rounds'], non_analytics_df['funding_rounds']),\n",
    "                                      statistic=statistic,\n",
    "                                      n_resamples=100,\n",
    "                                      vectorized=False)\n",
    "\n",
    "# Print the p-value\n",
    "print(perm_result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37f73b-3e2f-4a35-97f5-8cf5660b0765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443deaa9-c516-428f-8bcc-f79ea2f2afe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d86138-b87a-499a-8b16-d8890906f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad9861-056c-4bba-8306-34d543b3853d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82356c92-9b49-46e1-931a-9df1456666a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
